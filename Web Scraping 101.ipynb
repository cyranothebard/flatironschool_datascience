{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Quick-Google-Colab-Overview\" data-toc-modified-id=\"Quick-Google-Colab-Overview-1\">Quick Google Colab Overview</a></span></li><li><span><a href=\"#Web-Scraping-101\" data-toc-modified-id=\"Web-Scraping-101-2\">Web Scraping 101</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recommended-packages/tools-to-use\" data-toc-modified-id=\"Recommended-packages/tools-to-use-2.1\">Recommended packages/tools to use</a></span></li><li><span><a href=\"#Using-python's-requests-module:\" data-toc-modified-id=\"Using-python's-requests-module:-2.2\">Using python's <code>requests</code> module:</a></span></li><li><span><a href=\"#Random-Tips---Text-Printing/Formatting:**\" data-toc-modified-id=\"Random-Tips---Text-Printing/Formatting:**-2.3\">Random Tips - Text Printing/Formatting:**</a></span></li><li><span><a href=\"#Quick-Review----HTML-&amp;-Tags\" data-toc-modified-id=\"Quick-Review----HTML-&amp;-Tags-2.4\">Quick Review -  HTML &amp; Tags</a></span></li></ul></li><li><span><a href=\"#1)-Using-BeautifulSoup\" data-toc-modified-id=\"1)-Using-BeautifulSoup-3\">1) Using <code>BeautifulSoup</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Cook-a-soup\" data-toc-modified-id=\"Cook-a-soup-3.1\">Cook a soup</a></span></li><li><span><a href=\"#What's-in-a-Soup?\" data-toc-modified-id=\"What's-in-a-Soup?-3.2\">What's in a Soup?</a></span></li><li><span><a href=\"#Navigating-the-HTML-Family-Tree:-Children,-siblings,-and-parents\" data-toc-modified-id=\"Navigating-the-HTML-Family-Tree:-Children,-siblings,-and-parents-3.3\">Navigating the HTML Family Tree: Children, siblings, and parents</a></span></li><li><span><a href=\"#Searching-Through-Soup\" data-toc-modified-id=\"Searching-Through-Soup-3.4\">Searching Through Soup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Finding-the-target-tags-to-isolate\" data-toc-modified-id=\"Finding-the-target-tags-to-isolate-3.4.1\">Finding the target tags to isolate</a></span></li><li><span><a href=\"#Using-BeautifulSoup's-search-functions\" data-toc-modified-id=\"Using-BeautifulSoup's-search-functions-3.4.2\">Using BeautifulSoup's search functions</a></span></li></ul></li></ul></li><li><span><a href=\"#2)-Walk-through-example/code\" data-toc-modified-id=\"2)-Walk-through-example/code-4\">2) Walk-through example/code</a></span><ul class=\"toc-item\"><li><span><a href=\"#James'-Functions\" data-toc-modified-id=\"James'-Functions-4.1\">James' Functions</a></span></li><li><span><a href=\"#Walkthrough---using-James'-functions\" data-toc-modified-id=\"Walkthrough---using-James'-functions-4.2\">Walkthrough - using James' functions</a></span></li></ul></li><li><span><a href=\"#3)-Notes-from-group-practice-session(06/24/19):\" data-toc-modified-id=\"3)-Notes-from-group-practice-session(06/24/19):-5\">3) Notes from group practice session(06/24/19):</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqLBiwwB6I2k"
   },
   "source": [
    "# Quick Google Colab Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jirvingphd/my_data_science_notes/blob/master/Web_Scraping_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bn_zdpwZ6IfO"
   },
   "source": [
    "**Google Colab Quick - Notes**\n",
    " 1. Open the sidebar! (the little  ' > ' button on the top-left of the document pane.)\n",
    "    - Use `Table of Contents` to Jump between the 3 major sections.\n",
    "    - Mount your google drive via the `Files `\n",
    "    - Note: to make a section appear in the Table of Contents, create a NEW text cell for the *header only*. This will also let you collapse all of the cells in the section to reduce clutter.\n",
    " \n",
    "    \n",
    " 2. Google Colab already has most common python packages.\n",
    "    - You can pip install anything by prepending an exclamation point \n",
    "    ```python \n",
    "    !pip install bs_ds\n",
    "    !pip install fake_useragent\n",
    "    !pip install lxml\n",
    "    ```\n",
    "    \n",
    "3. Open a notebook from github or save to github using `File > Upload Notebook` and `File> Save a copy in github`, respectively\n",
    "\n",
    "4. Using GPUs/TPUs\n",
    "    - `Runtime > Change Runtime Type > Hardware Acceleration`\n",
    "\n",
    "5. Run-Before and Run-After\n",
    "    - Go to `Runtime` and select `Run before` to run all cells up to the currently active cell\n",
    "    - Go to `Runtime` and select `Run after` to run all cells that follow the currently active cell \n",
    "\n",
    "6. Cloud Files with Colab\n",
    "    - **Open .csv's stored in a github repo directly with Pandas**:\n",
    "        - Go to the repo on GitHub, click on the csv file, then click on `Download` or `Raw` which will then change to show you the raw text. Copy and paste the link in your address bar (should start with www.rawgithubusercontent).\n",
    "        - In your notebook, do `df=pd.read_csv(url)` to load in the data.\n",
    "    - **Google Drive: Open sidebar > Files> click Mount Drive**\n",
    "        - or use this function: \n",
    "        ```python \n",
    "        def mount_google_drive(force_remount=True):\n",
    "            from google.colab import drive\n",
    "            print('drive_filepath=\"drive/My Drive/\"')\n",
    "            return drive.mount('/content/drive', force_remount=force_remount)\n",
    "        ```\n",
    "        - Then access files by file path like usual.\n",
    "        \n",
    "    - Dropbox Files: (like images or csv)\n",
    "        - Copy and paste the share link.\n",
    "        - Change the end of the link from `dl=0`to `dl=1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAuVqqH66xeI"
   },
   "source": [
    "# Web Scraping 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Wnkf6zBwjsF"
   },
   "source": [
    "**Table of Contents - Shallow**\n",
    "1. Notes on Using BeautifulSoup\n",
    "2. Walk-through example/code\n",
    "    - My personal functions and then a working code frame using them.\n",
    "3. Notes Section for \n",
    " - After this, make sure to check out [Brandon's Web Scraping 202](https://github.com/cyranothebard/flatironschool_datascience/blob/master/Web%20Scraping%20202.ipynb)\n",
    " - He goes into using alternating ip addresses and more complex framework for harvesting content\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "## Recommended packages/tools to use\n",
    "1. `fake_useragent`\n",
    "    - pip-installable module that conveniently supplies fake user agent information to use in your request headers.\n",
    "    - recommended by udemy course\n",
    "2. `lxml`\n",
    "    - popular pip installable html parser (recommended by Udemy course)\n",
    "    - using `'html.parser'` in requests.get() did not work for me, I had to install lxml\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gh6WTxiVYqkh"
   },
   "outputs": [],
   "source": [
    "!pip install bs_ds\n",
    "!pip install fake_useragent\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaWIHHicwtAr"
   },
   "source": [
    "## Using python's `requests` module:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JpBMzWysPb0"
   },
   "source": [
    "\n",
    "-  Use `requests` library to initiate connections to a website.\n",
    "- Check the status code returned to determine if connection was successful (status code=200)\n",
    "\n",
    "```python\n",
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "\n",
    "# Connect to the url using requests.get\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "```\n",
    "\n",
    " ___\n",
    "| Status Code | Code Meaning \n",
    "| --------- | -------------|\n",
    "1xx |   Informational\n",
    "2xx|    Success \n",
    "3xx|     Redirection\n",
    "4xx|     Client Error \n",
    "5xx |    Server Error\n",
    "\n",
    "___\n",
    "- **Note: You can add a `timeout` to `requests.get()` to avoid indefinite waiting**\n",
    "    - Best in multiples of 3 (`timeout=3` or `6` , `9` ,etc.)\n",
    "\n",
    "```python\n",
    "# Add a timeout to prevent hanging\n",
    "response = requests.get(url, timeout=3)\n",
    "response.status_code\n",
    "\n",
    "```\n",
    "- **`response` is a dictionary with the contents printed below**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "id": "w5Rn-Y0N0q3B",
    "outputId": "eb7b1af9-2a9a-447a-cb7b-afcd1ce72ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code:  200\n",
      "Connection successfull.\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\tContents of Response.items():\n",
      "------------------------------------------------------------\n",
      "Date                     : Mon, 24 Jun 2019 15:13:12 GMT           \n",
      "Content-Type             : text/html; charset=UTF-8                \n",
      "Content-Length           : 64418                                   \n",
      "Connection               : keep-alive                              \n",
      "Server                   : mw1274.eqiad.wmnet                      \n",
      "X-Content-Type-Options   : nosniff                                 \n",
      "P3P                      : CP=\"This is not a P3P policy! See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"\n",
      "X-Powered-By             : HHVM/3.18.6-dev                         \n",
      "Content-language         : en                                      \n",
      "Last-Modified            : Mon, 24 Jun 2019 13:58:29 GMT           \n",
      "Backend-Timing           : D=137942 t=1561384720599492             \n",
      "Vary                     : Accept-Encoding,Cookie,Authorization,X-Seven\n",
      "Content-Encoding         : gzip                                    \n",
      "X-Varnish                : 458249968 437952989, 205788053 193766102\n",
      "Via                      : 1.1 varnish (Varnish/5.1), 1.1 varnish (Varnish/5.1)\n",
      "Age                      : 4471                                    \n",
      "X-Cache                  : cp1089 hit/10, cp1083 hit/1             \n",
      "X-Cache-Status           : hit-front                               \n",
      "Server-Timing            : cache;desc=\"hit-front\"                  \n",
      "Strict-Transport-Security: max-age=106384710; includeSubDomains; preload\n",
      "Set-Cookie               : WMF-Last-Access=24-Jun-2019;Path=/;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, WMF-Last-Access-Global=24-Jun-2019;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, GeoIP=US:VA::38.66:-77.25:v4; Path=/; secure; Domain=.wikipedia.org\n",
      "X-Analytics              : ns=0;page_id=52328;https=1;nocookies=1  \n",
      "X-Client-IP              : 35.237.177.107                          \n",
      "Cache-Control            : private, s-maxage=0, max-age=0, must-revalidate\n",
      "Accept-Ranges            : bytes                                   \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "\n",
    "response = requests.get(url, timeout=3)\n",
    "print('Status code: ',response.status_code)\n",
    "if response.status_code==200:\n",
    "    print('Connection successfull.\\n\\n')\n",
    "else: \n",
    "    print('Error. Check status code table.\\n\\n')    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Print out the contents of a request's response\n",
    "print(f\"{'---'*20}\\n\\tContents of Response.items():\\n{'---'*20}\")\n",
    "\n",
    "for k,v in response.headers.items():\n",
    "    print(f\"{k:{25}}: {v:{40}}\") # Note: add :{number} inside of a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "fbpLnJrzDetE",
    "outputId": "c1b0484f-4902-4114-ea8a-74d83623a5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: Mon, 24 Jun 2019 15:13:12 GMT\n",
      "Content-Type: text/html; charset=UTF-8\n",
      "Content-Length: 64418\n",
      "Connection: keep-alive\n",
      "Server: mw1274.eqiad.wmnet\n",
      "X-Content-Type-Options: nosniff\n",
      "P3P: CP=\"This is not a P3P policy! See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"\n",
      "X-Powered-By: HHVM/3.18.6-dev\n",
      "Content-language: en\n",
      "Last-Modified: Mon, 24 Jun 2019 13:58:29 GMT\n",
      "Backend-Timing: D=137942 t=1561384720599492\n",
      "Vary: Accept-Encoding,Cookie,Authorization,X-Seven\n",
      "Content-Encoding: gzip\n",
      "X-Varnish: 458249968 437952989, 205788053 193766102\n",
      "Via: 1.1 varnish (Varnish/5.1), 1.1 varnish (Varnish/5.1)\n",
      "Age: 4471\n",
      "X-Cache: cp1089 hit/10, cp1083 hit/1\n",
      "X-Cache-Status: hit-front\n",
      "Server-Timing: cache;desc=\"hit-front\"\n",
      "Strict-Transport-Security: max-age=106384710; includeSubDomains; preload\n",
      "Set-Cookie: WMF-Last-Access=24-Jun-2019;Path=/;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, WMF-Last-Access-Global=24-Jun-2019;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, GeoIP=US:VA::38.66:-77.25:v4; Path=/; secure; Domain=.wikipedia.org\n",
      "X-Analytics: ns=0;page_id=52328;https=1;nocookies=1\n",
      "X-Client-IP: 35.237.177.107\n",
      "Cache-Control: private, s-maxage=0, max-age=0, must-revalidate\n",
      "Accept-Ranges: bytes\n"
     ]
    }
   ],
   "source": [
    "for k,v in response.headers.items():\n",
    "    print(f\"{k}: {v}\") # Note: add :{number} inside of a  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAPbGwxTDZbG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjXVI4Qpw2_n"
   },
   "source": [
    "## Random Tips - Text Printing/Formatting:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkwN3WP80tiS"
   },
   "source": [
    "\n",
    "- **You can repeat strings by using multiplication**\n",
    "    - `'---'*20` will repeat the dashed lines 20 times\n",
    "\n",
    "- **You can determine how much space is alloted for a variable when using f-strings**\n",
    "    - Add a `:{##}` after the variable to specify the allocated width\n",
    "    - Add a `>` before the `{##}` to force alignment \n",
    "    - Add another symbol (like '.'' or '-') before `>` to add guiding-line/placeholder (like in a table of contents)\n",
    "\n",
    "```python\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(f\"Status code: {response.status_code:>{20}}\")\n",
    "print(f\"Status code: {response.status_code:->{20}}\")\n",
    "```    \n",
    "```\n",
    "# Returns:\n",
    "Status code: 200\n",
    "Status code:                  200\n",
    "Status code: -----------------200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hO3ac1hE8gr5"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSwUhVVTwvRq"
   },
   "source": [
    "## Quick Review -  HTML & Tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jZuk4QQ17nU"
   },
   "source": [
    "- All HTML pages have the following components\n",
    "    1. document declaration followed by html tag\n",
    "    \n",
    "    `<!DOCTYPE html>`<br>\n",
    "    `<html>`\n",
    "    2. Head\n",
    "     html tag<br>\n",
    "    `<head> <title></title></head>`\n",
    "    3. Body<br>\n",
    "    `<body>` ... content... `</body>`<br>\n",
    "    `</html>`\n",
    "\n",
    "- Html content is divdied into **tags** that specify the type of content.\n",
    "    - [Basic Tags Reference Table](https://www.w3schools.com/tags/ref_byfunc.asp)\n",
    "    - [Full Alphabetical Tag Reference Table](https://www.w3schools.com/tags/)\n",
    "    \n",
    "    - **tags** have attributes\n",
    "        - [Tag Attributes](https://www.w3schools.com/html/html_attributes.asp)\n",
    "        - Attributes are always defined in the start/opening tag. \n",
    "\n",
    "    - **tags** may have several content-creator-defined attributes such as `class` or `id`\n",
    "- We will **use the tag and its identifying attributes to isolate content** we want on a web page with BeautifulSoup.\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7H_4da2vkGL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "we-zGs8lw7kY"
   },
   "source": [
    "#  1) Using `BeautifulSoup`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDQT-D171lhn"
   },
   "source": [
    "\n",
    "## Cook a soup\n",
    "\n",
    "- Connect to a website using`response = requests.get(url)`\n",
    "- Feed `response.content` into BeautifulSoup \n",
    "- Must specify the parser that will analyze the contents\n",
    "    - default available is `'html.parser'`\n",
    "    - recommended is to install and use `lxml` [[lxml documentation](https://lxml.de/3.7/)]\n",
    "- use soup.prettify() to get a user-friendly version of the content to print\n",
    "\n",
    "```python\n",
    "# Define Url and establish connection\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "response = requests.get(url, timeout=3)\n",
    "\n",
    "# Feed the response's .content into BeauitfulSoup\n",
    "page_content = response.content\n",
    "soup = BeautifulSoup(page_content,'lxml') #'html.parser')\n",
    "\n",
    "# Preview soup contents using .prettify()\n",
    "print(soup.prettify()[:2000])\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FieLZ63VVEXi"
   },
   "source": [
    "## What's in a Soup?\n",
    "- **A soup is essentially a collection of `tag objects`**\n",
    "    - each tag from the html is a tag object in the soup\n",
    "    - the tag's maintain the hierarchy of the html page, so tag objects will contain _other_ tag objects that were under it in the html tree.\n",
    "\n",
    "- **Each tag has a:**\n",
    "    - `.name`\n",
    "    - `.contents`\n",
    "    - `.string`\n",
    "    \n",
    "- **A tag can be access by name (like a column in a dataframe using dot notation)**\n",
    "    - and then you can access the tags within the new tag-variable just like the first tag\n",
    "    ```python\n",
    "    # Access tags by name\n",
    "    meta = soup.meta\n",
    "    head = soup.head\n",
    "    body = soup.body\n",
    "    # and so on...\n",
    "    ```\n",
    "- [!] ***BUT this will only return the FIRST tag of that type, to access all occurances of a tag-type, we will need to navigate the html family tree***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ZCLylw9RkVL"
   },
   "source": [
    "\n",
    "## Navigating the HTML Family Tree: Children, siblings, and parents\n",
    "\n",
    "- **Each tag is located within a tree-hierarchy of parents, siblings, and children**\n",
    "    - The family-relation is based on the identation level of the tags.\n",
    "\n",
    "- **Methods/attributes for the location/related tags of a tag**\n",
    "    - `.parent`, `.parents`\n",
    "    - `.child`, `.children`\n",
    "    - `.descendents`\n",
    "    - `.next_sibling`, `.previous_sibling`\n",
    "\n",
    "- *Note: a newline character `\\n` is also considered a tag/sibling/child*\n",
    "\n",
    "#### Accessing Child Tags\n",
    "\n",
    "- To get to later occurances of a tag type (i.e. the 2nd `<p>` tag in a tree), we need to navigate through the parent tag's `children`\n",
    "    - To access an iterable list of a tag's children use `.children`\n",
    "        - But, this only returns its *direct children*  (one indentation level down)     \n",
    "        \n",
    "    ```python\n",
    "    # print direct children of the body tag\n",
    "    body = soup.body\n",
    "    for child in body.children:\n",
    "        # print child if its not empty\n",
    "        print(child if child is not None else ' ', '\\n\\n')  # '\\n\\n' for visual separation\n",
    "    ```\n",
    "- To access *all children* use `.descendents`\n",
    "    - Returns all chidren and children of children\n",
    "    ```python\n",
    "    for child in body.descendents:\n",
    "        # print all children/grandchildren, etc\n",
    "        print(child if child is not None else ' ','\\n\\n')  \n",
    "    ```\n",
    "    \n",
    "#### Accessing Parent tags\n",
    "\n",
    "- To access the parent of a tag use `.parent`\n",
    "```python\n",
    "title = soup.head.title\n",
    "print(title.parent.name)\n",
    "```\n",
    "\n",
    "- To get a list of _all parents_ use `.parents`\n",
    "```python\n",
    "title = soup.head.title\n",
    "for parent in title.parents:\n",
    "    print(parent.name)\n",
    "```\n",
    "\n",
    "#### Accessing Sibling tags\n",
    "- siblings are tags in the same tree indentation level\n",
    "- `.next_sibling`, `.previous_sibling`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXktQsDvWW0A"
   },
   "source": [
    "## Searching Through Soup\n",
    "\n",
    "\n",
    "### Finding the target tags to isolate\n",
    "Using example  from  [Wikipedia article](https://en.wikipedia.org/wiki/Stock_market)\n",
    "where we are trying to isolate the body of the article content.\n",
    "\n",
    "\n",
    "- **Examine the website using Chrome's inspect view.**\n",
    "\n",
    "    - Press F12 or right-click > inspect\n",
    "\n",
    "    - Use the mouse selector tool (top left button) to explore the web page content for your desired target\n",
    "        - the web page element will be highlighted on the page itself and its corresponding entry in the document tree.\n",
    "        - Note: click on the web page with the selector in order to keep it selected in the document tree\n",
    "\n",
    "    - Take note of any identifying attributes for the target tag (class, id, etc)\n",
    "<img src=\"https://drive.google.com/uc?export-download&id=1KifQ_ukuXFdnCh1Tz1rwzA_cWkB_45mf\" width=450>\n",
    "\n",
    "### Using BeautifulSoup's search functions\n",
    "Note: while the process below is a decent summary, there is more nuance to html/css tags than I personally have been able to digest. \n",
    "    - If something doesn't work as expected/explained, please verify in the documentation.\n",
    "        - [BeauitfulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautiful-soup-documentation)\n",
    "        - [docs for .find_all()](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)\n",
    "    \n",
    "- **BeautifulSoup has methods for searching through descendent-tags**\n",
    "    - `.find`\n",
    "    - `.find_all`\n",
    "    \n",
    "- **Using `.find_all()`**\n",
    "    - Searches through all descendent tags and returns a result set (list of tag objects)\n",
    "```python\n",
    "# How to get results from .find_all()\n",
    "results = soup.find_all(name, attrs, recursive, string, limit,**kwargs) `\n",
    "```        \n",
    "    - `.find_all()` parameters:\n",
    "        - `name` _(type of tags to consider)_\n",
    "            - only consider tags with this name \n",
    "                - Ex: 'a',  'div', 'p' ,etc.\n",
    "        - `atrrs`_(css attributes that you are looking for in your target tag)_\n",
    "            - enter an attribute such as the class or id as a string\n",
    "\n",
    "                `attrs='mw-content-ltr'`\n",
    "            - if passing more than one attribute, must use a dictionary:\n",
    "\n",
    "            `attrs={'class':'mw-content-ltr', 'id':'mw-content-text'}`\n",
    "        - `recursive`_(Default=True)_\n",
    "            - search all children (`True`)\n",
    "            - search only  direct children(`False`)\n",
    "\n",
    "        - `string`\n",
    "            - search for text _inside_ of tags instead of the tags themselves\n",
    "            - can be regular expression\n",
    "        - `limit`\n",
    "            - How many results you want it to return\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "jJFr-RC7vzI9",
    "outputId": "4d0527cd-1220-466a-e08e-06a76259e94b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake_useragent\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
      "Building wheels for collected packages: fake-useragent\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
      "Successfully built fake-useragent\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-0.1.11\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install fake_useragent\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUuCKNCdbYdS"
   },
   "source": [
    "# 2) Walk-through example/code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwhquyO3x9q7"
   },
   "source": [
    "    - James functions \n",
    "    - Functional code scraping wikipedia pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVfjE8RhHZq-"
   },
   "source": [
    "## James' Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xBlb5NuyeAC"
   },
   "source": [
    "- `soup = cook_soup_from_url(url)`\n",
    "    - make a beautiful soup from url\n",
    "-`soup_links = get_all_links(soup)`\n",
    "    - get all links from soup and return as a list.\n",
    "    \n",
    "-  `absolute_links = make_absolute_links(url, soup_links) `\n",
    "    - use If `soup_links` are relative links that do not include the website domain and start with '../' instead of 'https://www... ').\n",
    "    - then can use the `absolute_links` to make new soups to continue searching for your desired content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTXEdbjQ_7RD"
   },
   "outputs": [],
   "source": [
    "def mount_google_drive(force_remount=True):\n",
    "    from google.colab import drive\n",
    "    print('drive_filepath=\"drive/My Drive/\"')\n",
    "    return drive.mount('/content/drive', force_remount=force_remount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "aIolt_tWAQ-E",
    "outputId": "6b4491fd-f12b-4eea-a29b-726be19988de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive_filepath=\"drive/My Drive/\"\n",
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "mount_google_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYgkNr2mA63V"
   },
   "outputs": [],
   "source": [
    "drive_filepath=\"drive/My Drive/\"\n",
    "# import os\n",
    "# os.listdir(drive_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwR64_fYv3Up"
   },
   "outputs": [],
   "source": [
    "def cook_soup_from_url(url, parser='lxml',sleep_time=0):\n",
    "    \"\"\"Uses requests to retreive webpage and returns a BeautifulSoup made using lxml parser.\"\"\"\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    sleep(sleep_time)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # check status of request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Error: Status_code !=200.\\n status_code={response.status_code}')\n",
    "                        \n",
    "    c = response.content\n",
    "    # feed content into a beautiful soup using lxml\n",
    "    soup = BeautifulSoup(c,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RM3tQkqyuoe"
   },
   "outputs": [],
   "source": [
    "def get_all_links(soup):#,attr_kwds=None):\n",
    "    \"\"\"Finds all links inside of soup that have the attributes(attr_kwds),which will be used in soup.findAll(attrs=attr_kwds).\n",
    "    Returns a list of links.\n",
    "    tag_type = 'a' or 'href'\"\"\"\n",
    "    all_a_tags = soup.findAll('a',attrs=kwds) \n",
    "    link_list = []\n",
    "    for link in all_a_tags:\n",
    "        test_link = link.get('href')#,attr=kwds)\n",
    "#         test_link = link.get('href',attrs=kwds)\n",
    "        link_list.append(test_link)\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNjG2XYFyPIH"
   },
   "outputs": [],
   "source": [
    "def make_absolute_links(source_url, rel_link_list):\n",
    "    \"\"\"Accepts the source_url for the source page of the rel_link_list and uses urljoin to return a list of valid absolute links.\"\"\"\n",
    "    \n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    absolute_links=[]\n",
    "\n",
    "    # Create a for loop to loop through links and make absolute html paths\n",
    "    for link in rel_link_list:\n",
    "\n",
    "        # Get base url using a url pasers and the story_url at the beginning of the nb\n",
    "        abs_link = urljoin(source_url,link)    \n",
    "\n",
    "        #concatenate and append to a list \n",
    "        absolute_links.append(abs_link)\n",
    "    \n",
    "    return absolute_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zo1sJyNE6ZCb"
   },
   "outputs": [],
   "source": [
    "def cook_batch_of_soups(link_list, sleep_time=1): #,user_fun = extract_target_text):\n",
    "    \"\"\"Accepts a list of links to extract and save in a list of dictionaries of soups\n",
    "    with their relative url path as their key.\n",
    "    Set user_fun to None to just extract full soups without user_extract\"\"\"\n",
    "    from time import sleep\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    batch_of_soups = []\n",
    "    \n",
    "    for link in link_list:\n",
    "        soup_dict = {}\n",
    "        \n",
    "        \n",
    "        # turn the url path into the dictionary key/title\n",
    "        url_dict_key_path = urlparse(link).path\n",
    "        url_dict_key = url_dict_key_path.split('/')[-1]\n",
    "        \n",
    "        soup_dict['_url'] = link\n",
    "        soup_dict['path'] = url_dict_key\n",
    "\n",
    "        # make a soup from the current link\n",
    "        page_soup = cook_soup_from_url(link, sleep_time=sleep_time)\n",
    "        soup_dict['soup'] = page_soup\n",
    "\n",
    "        \n",
    "#         if user_fun!=None:\n",
    "#             ## ADDING USER-SPECIFIED EXTRACTION FUNCTION       \n",
    "#             user_output = user_fun(page_soup) #can add inputs to function\n",
    "#             soup_dict['user_extract'] = user_output\n",
    "        \n",
    "        # Add current page's soup to batch_of_soups list\n",
    "        batch_of_soups.append(soup_dict)\n",
    "        \n",
    "    return batch_of_soups\n",
    "\n",
    "\n",
    "def extract_target_text(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "    \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "    if attrs_dict==None:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "    else:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "    # if extracting from multiple tags\n",
    "    output=[]\n",
    "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "    if join_text == True:\n",
    "        output = ' '.join(output)\n",
    "\n",
    "    ## ADDING SAVING EACH \n",
    "    if save_files==True:\n",
    "        text = output #soup.body.string\n",
    "        filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "        soup_dict['filename'] = filename\n",
    "        with open(filename,'w+') as f:\n",
    "            f.write(text)\n",
    "        print(f'File  successfully saved as {filename}')\n",
    "\n",
    "    return  output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ODwEjKH92T5"
   },
   "outputs": [],
   "source": [
    "def pickled_soup(soups, save_location='./', pickle_name='exported_soups.pckl'):\n",
    "    import pickle\n",
    "    import sys\n",
    "    \n",
    "    filepath = save_location+pickle_name\n",
    "    \n",
    "    with open(filepath,'wb') as f:\n",
    "        pickle.dump(soups, f)\n",
    "        \n",
    "    return print(f'Soup successfully pickled. Stored as {filepath}.')\n",
    "\n",
    "def load_leftovers(filepath):\n",
    "    import pickle\n",
    "    \n",
    "    print(f'Opening leftovers: {filepath}')\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        leftover_soup = pickle.load(f)\n",
    "        \n",
    "    return leftover_soup\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ng-HN_rRymjI"
   },
   "source": [
    "## Walkthrough - using James' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "1Pg3VWWmypij",
    "outputId": "1b76df53-a624-4173-8269-f96029873080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/wiki/Rent_seeking', '/wiki/Rhine_capitalism', '/wiki/State-sponsored_capitalism', '/wiki/Global_capitalism', '/wiki/Perspectives_on_capitalism']\n",
      "['https://en.wikipedia.org/wiki/Rent_seeking', 'https://en.wikipedia.org/wiki/Rhine_capitalism', 'https://en.wikipedia.org/wiki/State-sponsored_capitalism', 'https://en.wikipedia.org/wiki/Global_capitalism', 'https://en.wikipedia.org/wiki/Perspectives_on_capitalism']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "soup = cook_soup_from_url(url,sleep_time=1)\n",
    "\n",
    "\n",
    "## Get all links that match are interal wikipedia redirects [yes?]\n",
    "kwds = {'class':'mw-redirect'}\n",
    "links = get_all_links(soup)#,kwds)\n",
    "\n",
    "\n",
    "# preview first 5 links\n",
    "print(links[:5])\n",
    "\n",
    "\n",
    "# Turn relative links into absolute links\n",
    "abs_links = make_absolute_links(url,links)\n",
    "print(abs_links[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "6tCwXLiN7UCk",
    "outputId": "d360207c-aa0f-40eb-e8f3-de1d0d90b6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input links: == # of soups in batch:\n",
      "5 == 5\n",
      "\n",
      "Each soup_dict has  dict_keys(['_url', 'path', 'soup'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 118,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting only the first 5 links to test\n",
    "abs_links_for_soups = abs_links[:5]\n",
    "\n",
    "\n",
    "# Cooking a batch of soups from those chosen links\n",
    "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
    "\n",
    "# batch_of_soups is a list as long as the input link_list\n",
    "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
    "\n",
    "# batch_of_soups is a list of soup-dictionaries\n",
    "soup_dict = batch_of_soups[0]\n",
    "print('Each soup_dict has ',soup_dict.keys())\n",
    "\n",
    "# the page's soup is stored under soup_dict['soup']\n",
    "soup_from_soup_dict = soup_dict['soup']\n",
    "type(soup_from_soup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFVnj20YmQK2"
   },
   "source": [
    "#### Notes on extracting content.\n",
    "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "QwU8aPHJhAVm",
    "outputId": "034b20bf-67f1-4f4f-8450-33ee0f4ab111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Rent-seeking is a concept in public choice theory as well as in economics, that involves seeking to increase one's share of existing wealth without creating new wealth. Rent-seeking results in reduced economic efficiency through misallocation of resources, reduced wealth-creation, lost government revenue, heightened income inequality,[1] and potential national decline.\n",
      " Attempts at capture of regulatory agencies to gain a coercive monopoly can result in advantages for the rent seeker in a market while imposing disadvantages on their incorrupt competitors. This is one of many possible forms of rent-seeking behavior.\n",
      " The idea of rent-seeking was developed by Gordon Tullock in 1967,[2] while the expression rent-seeking itself was coined in 1974 by Anne Krueger.[3] The word \"rent\" does not refer specifically to payment on a lease but rather to Adam Smith's division of incomes into profit, wage, and rent.[4] The origin of the term refers to gaining control of land or other natural resour\n"
     ]
    }
   ],
   "source": [
    "## ADDING extract_target_text to precisely target text\n",
    "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "#     if attrs_dict==None:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "#     else:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "#     # if extracting from multiple tags\n",
    "#     output=[]\n",
    "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "#     if join_text == True:\n",
    "#         output = ' '.join(output)\n",
    "\n",
    "#     ## ADDING SAVING EACH \n",
    "#     if save_files==True:\n",
    "#         text = output #soup.body.string\n",
    "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "#         soup_dict['filename'] = filename\n",
    "#         with open(filename,'w+') as f:\n",
    "#             f.write(text)\n",
    "#         print(f'File  successfully saved as {filename}')\n",
    "\n",
    "#     return  output\n",
    "\n",
    "# ####################\n",
    "\n",
    "## RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
    "for i, soup_dict in enumerate(batch_of_soups):\n",
    "    \n",
    "    # Get the soup from the dict\n",
    "    soup = soup_dict['soup']\n",
    "    \n",
    "    # Extract text \n",
    "    extracted_text = extract_target_text(soup)\n",
    "    \n",
    "    # Add key:value for results of extract\n",
    "    soup_dict['extracted'] = extracted_text\n",
    "    \n",
    "    # Replace the old soup_dict with the new one with 'extracted'\n",
    "    batch_of_soups[i] = soup_dict\n",
    "    \n",
    "example_extracted_text=batch_of_soups[0]['extracted']\n",
    "print(example_extracted_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2lgbOyKyg0-"
   },
   "source": [
    "___\n",
    "___\n",
    "\n",
    "# 3) Notes from group practice session(06/24/19):\n",
    "- note: most changes from our session were added to section 2 above, which now has updated code and additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUJ50zdSvjyL"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from fake_useragent import UserAgent\n",
    "# ua = UserAgent()\n",
    "\n",
    "# header = {'user-agent':ua.chrome}\n",
    "# print('Header:\\n',header)\n",
    "\n",
    "# url ='https://en.wikipedia.org/wiki/Stock_market'\n",
    "# response = requests.get(url, timeout=3, headers=header)\n",
    "\n",
    "# print('Status code: ',response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJwSP_bWnBL3"
   },
   "source": [
    "#### Example For Kate's Website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "texdGvNkOoY8"
   },
   "outputs": [],
   "source": [
    "url='http://www.temis.nl/uvradiation/archives/v2.0/overpass/uv_Bern_Switzerland.dat'\n",
    "batch_soups_kate = cook_batch_of_soups([url])\n",
    "\n",
    "## Saving each page's body as a text file\n",
    "for soup_dict in batch_soups_kate:\n",
    "    text = soup_dict['soup'].body.string\n",
    "    filename =f\"drive/My Drive/test_text_saving {soup_dict['url_dict_key']}.txt\"\n",
    "    with open(filename,'w+') as f:\n",
    "        f.write(text)\n",
    "\n",
    "## Loading in a file to test if working.\n",
    "test_file = batch_soups_kate[0].filename\n",
    "with open(filename,'r') as f:\n",
    "    data = f.read()\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Web Scraping 101.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
